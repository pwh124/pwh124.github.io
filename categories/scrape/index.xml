<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scrape on Paul Hook</title>
    <link>/categories/scrape/</link>
    <description>Recent content in Scrape on Paul Hook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Apr 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/scrape/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Scraping bioRxiv - Part 2: Navigating Javascript in R</title>
      <link>/blog/2018/04/17/2018-03-18-scraping-biorxiv-part-2-navigating-javascript-in-r/</link>
      <pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04/17/2018-03-18-scraping-biorxiv-part-2-navigating-javascript-in-r/</guid>
      <description>###tl;dr The area on a bioRxiv page that displays where the paper was published is populated using Javascript when the page is loaded. The R package RSelenium allows for the scraping of that dynamically loaded information, including the published paper’s DOI.
###Where I left off In my ongoing series of blog posts attempting to track the publishing of papers on bioRxiv, I left off with being able to scrape bioRxiv pages in R but failing to be able to extract the information about the published peer-reviewed version of the paper.</description>
    </item>
    
    <item>
      <title>Scraping bioRxiv</title>
      <link>/blog/2018/04/10/2018-02-27-scraping-biorxiv-getting-started/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018/04/10/2018-02-27-scraping-biorxiv-getting-started/</guid>
      <description>This post is the first of a series focusing on tracking preprints on bioRxiv. The motivation behind this and a more detailed explanantion can be found here.
###Scraping in R When I began to approach preprint tracking, the mysterious world of “HTML scraping” seemed like something that would help. Simply, scraping allows you to extract information from the underlying HTML (Hypertext Markup Language) documents that make up websites.
HTML scraping seemed like a good idea for preprint tracking because: A) It doesn’t seem like there are simple/easy ways to access a bioRxiv database that holds such information (especially for someone with limited experience in this like me) and B) Whether or not the paper is published in a peer-reviewed journal is displayed clearly on each preprint’s page (see below on the webpage in red).</description>
    </item>
    
  </channel>
</rss>